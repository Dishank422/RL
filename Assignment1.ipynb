{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DISHANK JAIN - AI20BTECH11011"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: Suppose we are given a Markov Reward Process <S, A, P, $\\gamma$>. We know that we can write $$V = R + \\gamma P V$$\n",
    "\n",
    "Now suppose the last state is an absorbing state. Thus $$P = \\begin{pmatrix}P_{1s'}^T\\\\P_{2s'}^T\\\\ \\vdots \\\\ P_{(n-1)s'}^T \\\\ P_{ns'}^T\\end{pmatrix}$$.\n",
    "\n",
    "where $P_{ns'}^T = \\begin{pmatrix}0 & 0 & ... & 1\\end{pmatrix}$. Now, at absorbing state, we already know that the value is same as the reward, i.e. $V_{n} = R_{n}$. Thus, to obtain the value at other states, we can also solve the equation $$\\begin{pmatrix}V_1\\\\ V_2\\\\ \\vdots\\\\ V_{n-1}\\end{pmatrix} = \\begin{pmatrix}R_1 \\\\ R_2 \\\\ \\vdots \\\\ R_{n-1}\\end{pmatrix} + \\gamma \\begin{pmatrix}P_{1s'}^T\\\\P_{2s'}^T\\\\ \\vdots \\\\ P_{(n-1)s'}^T \\end{pmatrix} \\begin{pmatrix}V_1 \\\\ V_2 \\\\ \\vdots \\\\ V_n\\end{pmatrix}$$.\n",
    "\n",
    "Now, note that the we can also add the equation $V_n = R_n$ to the above equation by writing $$\\begin{pmatrix}V_1\\\\ V_2\\\\ \\vdots\\\\ V_{n-1} \\\\ V_n\\end{pmatrix} = \\begin{pmatrix}R_1 \\\\ R_2 \\\\ \\vdots \\\\ R_{n-1} \\\\ R_n\\end{pmatrix} + \\gamma \\begin{pmatrix}P_{1s'}^T\\\\P_{2s'}^T\\\\ \\vdots \\\\ P_{(n-1)s'}^T \\\\ \\vec{0}^T\\end{pmatrix} \\begin{pmatrix}V_1 \\\\ V_2 \\\\ \\vdots \\\\ V_n\\end{pmatrix}$$\n",
    "\n",
    "Here, $\\vec{0}$ is the all zeros vector. Note that in the above equation, we are only adding the last row to make the matrix on the right into a square matrix. We already know $V_n$ and we are not solving for it. Now let $$P' = \\begin{pmatrix}P_{1s'}^T\\\\P_{2s'}^T\\\\ \\vdots \\\\ P_{(n-1)s'}^T \\\\ \\vec{0}^T\\end{pmatrix}$$ We can thus write $$V = (I-\\gamma P')^{-1}R$$\n",
    "I use this equation in many of the following questions to solve the MDP or MRP."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 1\n",
    "### Part a\n",
    "The states of the given Markov Reward Process are: $$\\{S, 1, 7, 3, 8, 5, 6, W\\}$$\n",
    "The transition matrix (P) for the given Markov reward process is\n",
    "\n",
    "\\begin{pmatrix}0 & 0.25 & 0.25 & 0.25 & 0.25 & 0 & 0 & 0\\\\ 0 & 0 & 0.25 & 0.25 & 0.25 & 0.25 & 0 & 0\\\\ 0 & 0 & 0.25 & 0.25 & 0.25 & 0 & 0 & 0.25\\\\ 0 & 0 & 0.25 & 0 & 0.25 & 0.25 & 0.25 & 0\\\\ 0 & 0 & 0 & 0.25 & 0.5 & 0 & 0 & 0.25\\\\ 0 & 0 & 0.25 & 0.25 & 0.25 & 0 & 0.25 & 0\\\\ 0 & 0 & 0.25 & 0.25 & 0.25 & 0 & 0 & 0.25\\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\end{pmatrix}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part b\n",
    "Let the reward be such that on landing on any state except state W, the agent gets the reward -1 else the reward is 0 only on state W. Thus the reward vector is $$R = \\begin{pmatrix} -1 \\\\ -1 \\\\ -1 \\\\ -1 \\\\ -1 \\\\ -1 \\\\ -1 \\\\ 0\\end{pmatrix}$$\n",
    "\n",
    "To find the expected number of steps, the discount factor ($\\gamma$) = 1. Now, the expected number of steps for each state is same as the value function of that state. We can compute this value function using the Bellman equation: $$V = (I-\\gamma P)^{-1}R$$\n",
    "\n",
    "We compute the same in the below code. We find that it takes about 7.08 steps to reach state W from state S."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of steps at each step = [ 7.08333333  7.          5.33333333  6.66666667  5.33333333  6.66666667\n",
      "  5.33333333 -0.        ]\n"
     ]
    }
   ],
   "source": [
    "P_prime = np.array([[0, 0.25, 0.25, 0.25, 0.25, 0, 0, 0], [0, 0, 0.25, 0.25, 0.25, 0.25, 0, 0], [0, 0, 0.25, 0.25, 0.25, 0, 0, 0.25], [0, 0, 0.25, 0, 0.25, 0.25, 0.25, 0], [0, 0, 0, 0.25, 0.5, 0, 0, 0.25], [0, 0, 0.25, 0.25, 0.25, 0, 0.25, 0], [0, 0, 0.25, 0.25, 0.25, 0, 0, 0.25], [0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "R = np.array([-1, -1, -1, -1, -1, -1, -1, 0])\n",
    "gamma = 1\n",
    "V = np.linalg.inv(np.eye(8)-gamma*P_prime)@R\n",
    "print(\"Expected number of steps at each step =\", -V)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 2\n",
    "### Part a\n",
    "For the given problem, there are (N+1) states corresponding to each value of m = 0, 1, 2, ..., N. There are two actions that can be taken at each state, a1 and a2, where a1 means to not call the repair man and a2 means to call the repair man. The transition matrix corresponding to action a1 is given by:\n",
    "\n",
    "$$P^{a1} = \\begin{pmatrix} 1 & 0 & ... & 0 \\\\ \\frac{1}{2} & \\frac{1}{2} & ... & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{1}{N+1} & \\frac{1}{N+1} & ... & \\frac{1}{N+1}\\end{pmatrix}$$\n",
    "\n",
    "The transition matrix corresponding to action a2 is given by:\n",
    "\n",
    "$$P^{a2} = \\begin{pmatrix} 0 & 0 & ... & 0 & 1 \\\\ 0 & 0 & ... & \\frac{1}{2} & \\frac{1}{2} \\\\ \\vdots & \\vdots & \\ddots  & \\vdots & \\vdots \\\\ 0 & \\frac{1}{N} & ... & \\frac{1}{N} & \\frac{1}{N} \\\\ \\frac{1}{N+1} & \\frac{1}{N+1} & ... & \\frac{1}{N+1} & \\frac{1}{N+1}\\end{pmatrix}$$\n",
    "\n",
    "Here, note that the reward does not depend on the final state but rather only on the initial state and the action taken. The reward matrix for action a1 is given by:\n",
    "\n",
    "$$R^{a1} = \\begin{pmatrix} 0 & 0 & ... & 0 \\\\ 1 & 1 & ... & 1 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ N & N & ... & N\\end{pmatrix}$$\n",
    "\n",
    "The reward matrix for action a2 is given by:\n",
    "\n",
    "$$R^{a2} = \\begin{pmatrix} -\\frac{N}{2} & -\\frac{N}{2} & ... & -\\frac{N}{2} \\\\ 1-\\frac{N}{2} & 1-\\frac{N}{2} & ... & 1-\\frac{N}{2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ N-\\frac{N}{2} & N-\\frac{N}{2} & ... & N-\\frac{N}{2}\\end{pmatrix}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part b\n",
    "I would use discounted reward for this setting because this is infinite horizon MDP."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part c\n",
    "We can compute the Value of the policy using Bellman equation. Note that for this particular policy, the MRP is finite horizon. Thus it is possible to use undiscounted setting. $$R^{\\pi}(s) = \\sum \\pi(a|s) \\sum_{s'} P_{ss'}^a R_{ss'}^a$$\n",
    "$$V^{\\pi} = (I-\\gamma P^{\\pi} V^{\\pi})^{-1}R^{\\pi}$$\n",
    "In the first equation, since $R_{ss'}^a$ does not depend on s', we can simplify and write $$R^{\\pi}(s) = R_{s, s'= 1}^a$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value vector for this policy is  [ 0.  2.  4.  6.  8. 10.]\n"
     ]
    }
   ],
   "source": [
    "P_prime = np.array([[0, 0, 0, 0, 0, 0], [1/2, 1/2, 0, 0, 0, 0], [1/3, 1/3, 1/3, 0, 0, 0], [1/4, 1/4, 1/4, 1/4, 0, 0], [1/5, 1/5, 1/5, 1/5, 1/5, 0], [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]])\n",
    "R = np.array([0, 1, 2, 3, 4, 5])\n",
    "gamma = 1\n",
    "V = np.linalg.inv(np.eye(6)-gamma*P_prime)@R\n",
    "print(\"The value vector for this policy is \", V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part d\n",
    "$$Q(s, a) = \\sum_{s'}P_{ss'}^a(R_{ss'}^a+\\gamma V^{\\pi}(s'))$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(s, a1) = [ 0.  2.  4.  6.  8. 10.]\n"
     ]
    }
   ],
   "source": [
    "P = np.array([[1, 0, 0, 0, 0, 0], [1/2, 1/2, 0, 0, 0, 0], [1/3, 1/3, 1/3, 0, 0, 0], [1/4, 1/4, 1/4, 1/4, 0, 0], [1/5, 1/5, 1/5, 1/5, 1/5, 0], [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]])\n",
    "R_a1 = np.array([[0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3], [4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5]])\n",
    "P_R = np.sum(P*R_a1, 1)\n",
    "P_V = np.sum(P*V, 1)\n",
    "print(\"Q(s, a1) =\", P_R+P_V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(s, a2) = [7.5 7.5 7.5 7.5 7.5 7.5]\n"
     ]
    }
   ],
   "source": [
    "P = np.array([[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 1/2, 1/2], [0, 0, 0, 1/3, 1/3, 1/3], [0, 0, 1/4, 1/4, 1/4, 1/4], [0, 1/5, 1/5, 1/5, 1/5, 1/5], [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]])\n",
    "R_a2 = np.array([[0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3], [4, 4, 4, 4, 4, 4], [5, 5, 5, 5, 5, 5]]) - 5/2\n",
    "P_R = np.sum(P * R_a2, 1)\n",
    "P_V = np.sum(P * V, 1)\n",
    "print(\"Q(s, a2) =\", P_R + P_V)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Therefore the optimal policy is:\n",
    "$$\\pi = argmax_{a}(Q(s, a1), Q(s, a2)) = \\begin{pmatrix}a2 \\\\ a2 \\\\ a2 \\\\ a2 \\\\ a1 \\\\ a1\\end{pmatrix}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 3\n",
    "### Part a\n",
    "The states for this MDP are {A, B, C, D}.\n",
    "$$P^{\\pi}(s'|s) = \\sum_{a}\\pi(a|s)P_{ss'}^a$$\n",
    "For policy 1:\n",
    "$$P^{\\pi_1} = \\begin{pmatrix}0 & 0.9 & 0.1 & 0 \\\\ 0.1 & 0 & 0 & 0.9 \\\\ 0.9 & 0 & 0 & 0.1 \\\\ 0 & 0 & 0 & 1\\end{pmatrix}$$\n",
    "$$P^{\\pi_2} = \\begin{pmatrix}0 & 0.1 & 0.9 & 0 \\\\ 0.9 & 0 & 0 & 0.1 \\\\ 0.1 & 0 & 0 & 0.9 \\\\ 0 & 0 & 0 & 1\\end{pmatrix}$$\n",
    "$$P^{\\pi_3} = \\begin{pmatrix}0 & 0.42 & 0.58 & 0 \\\\ 0.1 & 0 & 0 & 0.9 \\\\ 0.1 & 0 & 0 & 0.9 \\\\ 0 & 0 & 0 & 1\\end{pmatrix}$$\n",
    "$$R^{\\pi} = \\begin{pmatrix}-10\\\\-10\\\\-10\\\\100\\end{pmatrix}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of policy 1 = [ 75.6097561   87.56097561  68.04878049 100.        ]\n",
      "Value of policy 2 = [ 75.6097561   68.04878049  87.56097561 100.        ]\n",
      "Value of policy 3 = [ 77.77777778  87.77777778  87.77777778 100.        ]\n"
     ]
    }
   ],
   "source": [
    "P1_prime = np.array([[0, 0.9, 0.1, 0],[0.1, 0, 0, 0.9],[0.9, 0, 0, 0.1],[0, 0, 0, 0]])\n",
    "P2_prime = np.array([[0, 0.1, 0.9, 0],[0.9, 0, 0, 0.1],[0.1, 0, 0, 0.9],[0, 0, 0, 0]])\n",
    "P3_prime = np.array([[0, 0.42, 0.58, 0], [0.1, 0, 0, 0.9], [0.1, 0, 0, 0.9], [0, 0, 0, 0]])\n",
    "R = np.array([-10, -10, -10, 100])\n",
    "V1 = np.linalg.inv(np.eye(4)-gamma*P1_prime)@R\n",
    "V2 = np.linalg.inv(np.eye(4)-gamma*P2_prime)@R\n",
    "V3 = np.linalg.inv(np.eye(4)-gamma*P3_prime)@R\n",
    "print(\"Value of policy 1 =\", V1)\n",
    "print(\"Value of policy 2 =\", V2)\n",
    "print(\"Value of policy 3 =\", V3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part b\n",
    "Policy 3 is the best among the given three policies because the value for all states due to this policy is greater than that due to other policies."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part c\n",
    "No, all policies are not comparable. Value of state B is greater in Policy 1 and Value of state C is greater in Policy 2. Thus policies 1 and 2 cannot be compared."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part d\n",
    "Let the new policy be\n",
    "$$\\pi(s) = \\begin{cases}\\pi_1(s)\\; if\\; V^{\\pi_1}(s) > V^{\\pi_2}(s) \\\\ \\pi_2(s)\\; otherwise\\end{cases}$$\n",
    "At any state, we can say that the policy which has a higher value is better than the other policy. Thus we can follow that policy. Once we reach the next state, because of the Markov property, we can decide the action based solely on the state on which we currently are. Thus we can again follow the policy that has higher value on this new state."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 4\n",
    "### Part a\n",
    "1. **Low $\\gamma$, Low $\\eta$:** Since the agent is short-sighted, the agent will prefer the close exit. Since the noise in the environment is low, the agent will risk the cliff. Thus the agent will prefer the close exit while risking the cliff.\n",
    "2. **Low $\\gamma$, High $\\eta$:** Since the agent is short-sighted, the agent will prefer the close exit. But since the noise in the environment is high, the agent will try to avoid risking the cliff. Thus the agent will prefer the close exit while avoiding the cliff.\n",
    "3. **High $\\gamma$, Low $\\eta$:** Since the agent is now far-sighted, the agent will prefer the farther exit. Since the noise in the environment is low, the agent will risk the cliff. Thus the agent will prefer the distant exit while risking the cliff.\n",
    "4. **High $\\gamma$, High $\\eta$** Since the agent is far sighted, the agent will prefer the farther exit. But since the noise in the environment is high, the agent will avoid risking the cliff. Thus the agent will prefer the distant exit while avoiding the cliff.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 5\n",
    "### Part a\n",
    "$$P^{\\pi}(s'|s) = \\sum_a \\pi(a|s)P_{ss'}^a$$ Thus $P^{\\pi}$ is same for $M_1, M_2 \\; and \\; M_3$. $$R^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'}P_{ss'}^aR_{ss'}^a$$ Thus for $M_3$, $$R_3^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'}P_{ss'}^a(R_{1ss'}^a+R_{2ss'}^a)$$ $$ \\implies R_3^{\\pi} = R_1^{\\pi} + R_2^{\\pi}$$\n",
    "\n",
    "$$V^{\\pi} = (I-\\gamma P^{\\pi})R^{\\pi}$$ $$ \\implies V_3^{\\pi} = (I-\\gamma P^{\\pi})(R_1^{\\pi}+R_2^{\\pi}) = V_1^{\\pi}+V_2^{\\pi}$$\n",
    "\n",
    "$$Q^{\\pi}(s, a) = \\sum_{s'} P_{ss'}^a(R_{ss'}^a + \\gamma V^{\\pi}(s'))$$\n",
    "Thus for $M_3$, $$Q_3^{\\pi}(s, a) = \\sum_{s'} P_{ss'}^a((R_{1ss'}^a+R_{2ss'}^a) + \\gamma (V_1^{\\pi}(s') + V_2^{\\pi}(s'))) = Q_1^{\\pi}(s, a) + Q_2^{\\pi}(s, a)$$\n",
    "$$\\implies Q_3^{\\pi} = Q_1^{\\pi}+Q_2^{\\pi}$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part b\n",
    "No we cannot combine these two optimal policies in a simple manner to formulate an optimal policy $\\pi_{3}^*$ corresponding to $M_3$. This is because the actions we take at any state in policies $\\pi_1^*$ and $\\pi_2^*$ may not include the optimal action we need to take in that state in $M_3$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part c\n",
    "Yes, $\\pi^*$ will be an optimal policy for $M_3$. Since $\\pi^*$ is optimal policy in $M_1$ and $M_2$, we can see that\n",
    "$$V^{\\pi^*}_1(s) = \\max_aQ^{\\pi^*}_1(s, a)$$\n",
    "$$V^{\\pi^*}_2(s) = \\max_aQ^{\\pi^*}_2(s, a)$$\n",
    "$$V^{\\pi^*}_3(s) = V_1^{\\pi^*}(s)+V_2^{\\pi^*}(s) = \\max_aQ^{\\pi^*}_1(s, a)+\\max_aQ^{\\pi^*}_2(s, a) \\ge \\max_a(Q^{\\pi^*}_1(s, a)+Q^{\\pi^*}_2(s, a)) = \\max_aQ^{\\pi^*}_3(s, a)$$\n",
    "We know $V(s) = \\mathbb{E}_{\\pi}[Q(s, a)] \\le \\max_aQ(s, a)$. Thus,\n",
    "$$V^{\\pi^*}_3(s) = \\max_aQ^{\\pi^*}_3(s, a)$$\n",
    "Thus $\\pi^*$ is an optimal policy for $M_3$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Part d\n",
    "$$V_1 = R_1 + \\gamma PV_1$$\n",
    "$$V_2 = R_2 + \\gamma PV_2$$\n",
    "$$R_1 = R_2 + \\epsilon \\vec{1}$$\n",
    "Proposition is that $$V_1 = V_2 + \\dfrac{\\epsilon}{1-\\gamma}\\vec{1}$$\n",
    "We check if this value fits in equation 1.\n",
    "$$R_1+\\gamma PV_1 = R_2 + \\epsilon \\vec{1} + \\gamma P \\left(V_2 + \\dfrac{\\epsilon}{1-\\gamma}\\vec{1}\\right) = (R_2 + \\gamma PV_2) + \\left(\\epsilon \\vec{1} + \\dfrac{\\epsilon \\gamma}{1-\\gamma}P\\vec{1}\\right)$$\n",
    "$$P\\vec{1} = \\vec{1}$$\n",
    "$$\\implies R_1+\\gamma PV_1 = V_2 + \\dfrac{\\epsilon}{1-\\gamma}\\vec{1} = V_1$$\n",
    "Thus our proposition is correct. Therefore,\n",
    "$$V_1 = V_2 + \\dfrac{\\epsilon}{1-\\gamma}\\vec{1}$$"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}